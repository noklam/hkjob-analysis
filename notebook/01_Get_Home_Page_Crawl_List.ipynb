{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:19.307527Z",
     "start_time": "2019-06-10T03:08:19.230422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:21.535748Z",
     "start_time": "2019-06-10T03:08:19.320216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:21.607971Z",
     "start_time": "2019-06-10T03:08:21.537704Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install PyFunctional\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:21.675315Z",
     "start_time": "2019-06-10T03:08:21.609924Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from functional import seq\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:21.744611Z",
     "start_time": "2019-06-10T03:08:21.677268Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def has_href(x):\n",
    "    return hasattr(x, \"href\")\n",
    "\n",
    "def extract_href(x):\n",
    "    return x.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:22.506048Z",
     "start_time": "2019-06-10T03:08:21.749493Z"
    }
   },
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    class_no_jobs = \"_1f-toO_\" # Subject to change\n",
    "    class_jobs_pane = \"_35U47DP\"\n",
    "    HOME_PAGE = \"https://hk.jobsdb.com/hk/jobs/information-technology/1\"\n",
    "    BASE_LINK = \"https://hk.jobsdb.com/hk/jobs/information-technology/\"\n",
    "    req = requests.get(HOME_PAGE)\n",
    "    soup = BeautifulSoup(req.content, \"lxml\")\n",
    "    jobs_per_page = 30\n",
    "        \n",
    "    def get_numbers_of_job(self, soup:BeautifulSoup):\n",
    "            string = soup.find(\"span\", {\"class\": self.class_no_jobs}).parent.get_text() # '1-30 of 7634 jobs'\n",
    "            pattern = re.compile(r'.+ of (\\d+) jobs') \n",
    "            search_group = pattern.search(string)\n",
    "            self.no_jobs = int(search_group.group(1)) # Extract the first group, group(0) is full match\n",
    "            if self.no_jobs:                \n",
    "                print(f\"There are {self.no_jobs} jobs\")\n",
    "            else:\n",
    "                print('Maybe site is changed? cannot locate the numbers of jobs')\n",
    "                \n",
    "    def get_numbers_of_page(self):\n",
    "        if self.no_jobs is None:\n",
    "            self.get_numbers_of_job(self.soup)\n",
    "        self.no_pages = self.no_jobs // self.jobs_per_page + 1  \n",
    "        print(f\"There are {self.no_pages} pages\")\n",
    "        return self.no_jobs\n",
    "    \n",
    "    def is_job_href(self, href:str):\n",
    "        # https://hk.jobsdb.com/hk/en/job/associate-avp-vp-debt-capital-market-100003007140188\n",
    "        return href.startswith(\"https://hk.jobsdb.com/hk/en/job/\")\n",
    "    \n",
    "    def parse_href(self, soup:BeautifulSoup):\n",
    "#         print(\"parse job\")\n",
    "        tag_jobs_list = soup.select(\"#contentContainer\")[0].select(\"div > span > a\")\n",
    "        # Adopting some functional programming style here\n",
    "        href_list = (seq(tag_jobs_list)\n",
    "                     .filter(has_href)\n",
    "                     .map(extract_href)\n",
    "                    )\n",
    "        return list(href_list)\n",
    "            \n",
    "    def execute(self):\n",
    "        print('Start Crawling')\n",
    "        \n",
    "        jobs_list = []\n",
    "        current_page = 1\n",
    "        page_to_crawl = self.BASE_LINK + str(current_page)\n",
    "                \n",
    "        req = requests.get(page_to_crawl)\n",
    "        soup = BeautifulSoup(req.content, \"lxml\")              \n",
    "        \n",
    "        for current_page in tqdm(range(1, self.no_pages + 1)):\n",
    "            jobs_list = jobs_list + self.parse_href(soup)\n",
    " \n",
    "        print(\"Finish\")         \n",
    "        print(f\"We Crawl {len(jobs_list)} jobs\")\n",
    "        return jobs_list\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:22.623717Z",
     "start_time": "2019-06-10T03:08:22.508001Z"
    }
   },
   "outputs": [],
   "source": [
    "crawler = Crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:22.745715Z",
     "start_time": "2019-06-10T03:08:22.627622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7344 jobs\n",
      "There are 245 pages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7344"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler.get_numbers_of_job(crawler.soup) # Dirty Hack\n",
    "crawler.get_numbers_of_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:26.939961Z",
     "start_time": "2019-06-10T03:08:22.748645Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Crawling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2094bc1e5c94e5fad3ed13102e9dd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=245), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finish\n",
      "We Crawl 7350 jobs\n"
     ]
    }
   ],
   "source": [
    "crawler.jobs_list = crawler.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:27.666500Z",
     "start_time": "2019-06-10T03:08:26.941914Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def test_get_page_response_200(request: requests.models.Response):\n",
    "    assert request.status_code == 200\n",
    "    \n",
    "HOME_PAGE = \"https://hk.jobsdb.com/hk/jobs/information-technology/1\"\n",
    "req = requests.get(HOME_PAGE)\n",
    "test_get_page_response_200(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:27.759219Z",
     "start_time": "2019-06-10T03:08:27.671379Z"
    }
   },
   "outputs": [],
   "source": [
    "crawler.is_job_href??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:27.829492Z",
     "start_time": "2019-06-10T03:08:27.761172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler.is_job_href(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:08:27.949536Z",
     "start_time": "2019-06-10T03:08:27.832417Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_is_job_href():\n",
    "    crawler = Crawler()\n",
    "    href = \"https://hk.jobsdb.com/hk/en/job/associate-avp-vp-debt-capital-market-100003007140188\"\n",
    "    assert  crawler.is_job_href(href)\n",
    "    \n",
    "test_is_job_href()\n",
    "    \n",
    "  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.2",
    "jupytext_version": "1.1.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
