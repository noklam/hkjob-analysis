{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:35:47.064166Z",
     "start_time": "2019-06-09T11:35:47.054193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T18:45:17.228596Z",
     "start_time": "2019-06-09T18:45:17.138531Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from functional import seq\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T18:48:57.028791Z",
     "start_time": "2019-06-09T18:48:56.168093Z"
    }
   },
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    class_no_jobs = \"_1f-toO_\" # Subject to change\n",
    "    class_jobs_pane = \"_35U47DP\"\n",
    "    HOME_PAGE = \"https://hk.jobsdb.com/hk/jobs/information-technology/1\"\n",
    "    BASE_LINK = \"https://hk.jobsdb.com/hk/jobs/information-technology/\"\n",
    "    req = requests.get(HOME_PAGE)\n",
    "    soup = BeautifulSoup(req.content)\n",
    "    jobs_per_page = 30\n",
    "        \n",
    "    def get_numbers_of_job(self, soup:BeautifulSoup):\n",
    "            string = soup.find(\"span\", {\"class\": self.class_no_jobs}).parent.get_text() # '1-30 of 7634 jobs'\n",
    "            pattern = re.compile(r'.+ of (\\d+) jobs') \n",
    "            search_group = pattern.search(string)\n",
    "            self.no_jobs = int(search_group.group(1)) # Extract the first group, group(0) is full match\n",
    "            if self.no_jobs:                \n",
    "                print(f\"There are {self.no_jobs} jobs\")\n",
    "            else:\n",
    "                print('Maybe site is changed? cannot locate the numbers of jobs')\n",
    "                \n",
    "    def get_numbers_of_page(self):\n",
    "        if self.no_jobs is None:\n",
    "            self.get_numbers_of_job(self.soup)\n",
    "        self.no_pages = self.no_jobs // self.jobs_per_page + 1  \n",
    "        print(f\"There are {self.no_pages} pages\")\n",
    "        return self.no_jobs\n",
    "    \n",
    "    def is_job_href(self, href:str):\n",
    "        # https://hk.jobsdb.com/hk/en/job/associate-avp-vp-debt-capital-market-100003007140188\n",
    "        return href.startwith(\"https://hk.jobsdb.com/hk/en/jobs/\")\n",
    "    \n",
    "    def parse_href(self, soup:BeautifulSoup):\n",
    "#         print(\"parse job\")\n",
    "        tag_jobs_list = soup.select(\"#contentContainer\")[0].select(\"div > span > a\")\n",
    "        # Adopting some functional programming style here\n",
    "        href_list = (seq(tag_jobs_list)\n",
    "                     .filter(has_href)\n",
    "                     .map(extract_href)\n",
    "                    )\n",
    "        return list(href_list)\n",
    "            \n",
    "    def execute(self):\n",
    "        print('Start Crawling')\n",
    "        \n",
    "        jobs_list = []\n",
    "        current_page = 1\n",
    "        page_to_crawl = self.BASE_LINK + str(current_page)\n",
    "                \n",
    "        req = requests.get(page_to_crawl)\n",
    "        soup = BeautifulSoup(req.content)              \n",
    "        \n",
    "        for current_page in tqdm(range(1, self.no_pages + 1)):\n",
    "            jobs_list = jobs_list + self.parse_href(soup)\n",
    " \n",
    "        print(\"Finish\")         \n",
    "        print(f\"We Crawl {len(jobs_list)} jobs\")\n",
    "        return jobs_list\n",
    "#             break\n",
    "\n",
    "        \n",
    "            \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T18:48:57.184388Z",
     "start_time": "2019-06-09T18:48:57.030785Z"
    }
   },
   "outputs": [],
   "source": [
    "p = soup.select(\"#contentContainer\")[0].select(\"div > span > a\")\n",
    "# filter non href > list extract get href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T18:48:57.270146Z",
     "start_time": "2019-06-09T18:48:57.186372Z"
    }
   },
   "outputs": [],
   "source": [
    "def has_href(x):\n",
    "    return hasattr(x, \"href\")\n",
    "\n",
    "def extract_href(x):\n",
    "    return x.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T18:48:57.351927Z",
     "start_time": "2019-06-09T18:48:57.273137Z"
    }
   },
   "outputs": [],
   "source": [
    "crawler = Crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T18:48:57.450663Z",
     "start_time": "2019-06-09T18:48:57.354919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7634 jobs\n",
      "There are 255 pages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7634"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawler.get_numbers_of_job(soup)\n",
    "crawler.get_numbers_of_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.jobs_per_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-09T18:52:48.305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Crawling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89324819f8c54d91843f2370eab13d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=255), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n",
      "parse job\n"
     ]
    }
   ],
   "source": [
    "crawler.jobs_list = crawler.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T15:59:20.186121Z",
     "start_time": "2019-06-09T15:59:20.091311Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_get_page_response_200(request: requests.models.Response):\n",
    "    assert request.status_code == 200\n",
    "\n",
    "test_get_page_response_200(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T16:47:09.290191Z",
     "start_time": "2019-06-09T16:47:09.196435Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_is_job_href():\n",
    "    crawler = Crawler()\n",
    "    href = \"https://hk.jobsdb.com/hk/en/job/associate-avp-vp-debt-capital-market-100003007140188\"\n",
    "    assert  crawler.is_job_href(href)\n",
    "    \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
